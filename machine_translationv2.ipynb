{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Machine Translation\n",
    "Por Alexander Siavichay\n",
    "\n",
    "En este notebook se implementará 5 modelos para realizar Machine Translation, basado en el notebook original para iniciar el aprendizaje de Machine Translation\n",
    "\n",
    "\n",
    "## Introducción\n",
    "A continuación se elebaorará una red neuronal que funciona como parte de un maquina para traducción basado en pipeline, que es una técnica de implementación por medio de la cual se puede traslapar la ejecución de instrucciones. En la actualidad la segmentación es una de las tecnologías utilizadas para hacer procesadores más rápidos[1].\n",
    "\n",
    "Se pasará como dato de ingreso texto en Inglés, y retornará la traducción e francés.\n",
    "\n",
    "- **Preproceso** - Se convertirá el texto en sequencias de enteros.\n",
    "- **Modelos** - Se elaborará modelos que acepten un secuencia de enteros como entrada y retorne un distribución de probabilidad para una posible traducción.\n",
    "- **Predicción** Correr el modelo con el texto en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de acceso al GPU\n",
    "En el caso de disponer de un equipo con GPU (Se sugiere para que el proceso de aprendizaje no tome demasiado tiempo), se provee el ´codigo para verificar el hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_memory_growth\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Se entretará y evaluará el pipeline de [WMT](http://www.statmt.org/) con una muestra que contiene un pequeño vocabulario. \n",
    "\n",
    "### Carga de datos\n",
    "Se localizará los datos en `data/small_vocab_en` y `data/small_vocab_fr`. El archivo `small_vocab_en` contiene las sentencias en Inglés con sus traducciones en Francés en el archivo `small_vocab_fr`. \n",
    "\n",
    "A continuación se cargan estos archivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos\n",
    "Muestra de los archivos  `small_vocab_en` en Inglés con la traducción e Francés del archivo `small_vocab_fr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los textos han sido preprocesados. Las puntuaciones se han delimitado utilizando espacios. Se han convertido en minúsculas.\n",
    "\n",
    "### Vocabulario\n",
    "La complejidad del vocabulario genera la complejidad del problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproceso\n",
    "Los textos se convertirán en secuencias de enteros usando:\n",
    "\n",
    "1. Tokenize que generará los identificaodres de las palabras\n",
    "2. Add padding para que todas las secuencias tengan la misma longitud.\n",
    "\n",
    "### Tokenize (IMPLEMENTACIÓN)\n",
    "\n",
    "Los datos son convertidos en números para que sean procesados dentro de la red neuronal.\n",
    "\n",
    "Cadad secuencia se puede convertir en secuencia de palabras usando la función Keras [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(x)\n",
    "    text_tokenized = text_tokenizer.texts_to_sequences(x)\n",
    "    return text_tokenized, text_tokenizer\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTACION)\n",
    "Se realiza un proceso mediante el cual cada palabra se le asigna la misma longitud. Las sentencias son dinámicas en longitud, por ello es necesario normalizar su longitud.\n",
    "Para esto utilizamos la función [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    if length == None:\n",
    "        length = max([len(sentance) for sentance in x])\n",
    "    x = np.array(x)\n",
    "    x_padded = pad_sequences(x, maxlen=length, dtype='int32', padding='post')\n",
    "    return x_padded\n",
    "\n",
    "    return None\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproceso Pipeline\n",
    "El objetivo es elaborar una arquitectura de red neuronal. La función `preprocess` realiza este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "A continuación tenemos la parte medular del ddocumento, que es, generara arquitecturas de red neuronal. Se empezará con una red sencilla para luego ir modificando e incrementando características a la arquitectura.\n",
    "\n",
    "- Modelo 1 es una simple RNN\n",
    "- Modelo 2 is a RNN se le agrega Embedding\n",
    "- Modelo 3 es una RNN Bidirectional\n",
    "- Modelo 4 es una RNN Encoder-Decoder \n",
    "\n",
    "Al final se construye una arquitectura de red personalizada.\n",
    "\n",
    "### Retornando texto a partir de los identificadores Ids\n",
    "Se utiliza la función `logits_to_text` como puente entre la lógica enre el texto en Inglés y Francés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1: RNN (IMPLEMENTACION)\n",
    "![RNN](images/rnn.png)\n",
    "Se presenta un modelo básico que traduce el Inglés al Francés.\n",
    "La primera arquitectura que se presenta utiliza la clase Sequential de Keras. Se basa en ir agregando capas de manera lineal. Es importante indicarle la forma de la entrada. Luego el model creado es compilado con el método `complile`. Aquí le indicamos parámetros como la métrica `accuracy`. Recordemos que se pasaron las palabras a números, por lo tanto utilizaremos Numpy para su tratamiento mediante la función `fit`[2].\n",
    "\n",
    "La función softmax, es una función de activación que devuelve los números de ingreso en probabilidades que suman uno. Es una regresión logística, cuyo resultado es una distribución de probabilidad. Por lo tanto, el resultado será obtenido en un rango de 0 a 1. Computa la pérdida que experimentan los datos en un entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 21, 256)           198144    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 815,962\n",
      "Trainable params: 815,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 321s 3ms/step - loss: 1.9446 - accuracy: 0.5410 - val_loss: 1.2817 - val_accuracy: 0.6328\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 314s 3ms/step - loss: 1.2302 - accuracy: 0.6423 - val_loss: 1.0934 - val_accuracy: 0.6744\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 316s 3ms/step - loss: 1.0906 - accuracy: 0.6670 - val_loss: 0.9812 - val_accuracy: 0.6862\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 312s 3ms/step - loss: 1.0023 - accuracy: 0.6822 - val_loss: 0.9154 - val_accuracy: 0.6897\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 1596s 14ms/step - loss: 0.9504 - accuracy: 0.6908 - val_loss: 0.8649 - val_accuracy: 0.7129\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 316s 3ms/step - loss: 0.9033 - accuracy: 0.7002 - val_loss: 0.8774 - val_accuracy: 0.6880\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 315s 3ms/step - loss: 0.8761 - accuracy: 0.7042 - val_loss: 0.7795 - val_accuracy: 0.7298\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 313s 3ms/step - loss: 0.8398 - accuracy: 0.7133 - val_loss: 0.7573 - val_accuracy: 0.7387\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 312s 3ms/step - loss: 0.7896 - accuracy: 0.7294 - val_loss: 0.7896 - val_accuracy: 0.7214\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 312s 3ms/step - loss: 0.8213 - accuracy: 0.7102 - val_loss: 0.7221 - val_accuracy: 0.7422\n",
      "new jersey est parfois parfois en l' et il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "#simple_rnn_model = simple_model(\n",
    "#    tmp_x.shape,\n",
    "#    max_french_sequence_length-1,\n",
    "#    english_vocab_size,\n",
    "#    french_vocab_size)\n",
    "\n",
    "\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois parfois en l' et il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis\n",
    "El primer modelo nos presenta un accuracy de 72%, cabe indicar que el batchsize es igual a 1024, con ello mejoramos el tiempo de procesado pero, al observar los resultados, se verifica un bajo rendimiento.\n",
    "\n",
    "A continuación se trabajará en estos aspectos con la finalidad de mejorar el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2: Embedding (IMPLEMENTACION)\n",
    "![RNN](images/embedding.png)\n",
    "En el primer modelo se trabajó con ids numéricos, pero, existe una mejor forma de representar las palabras mediante Word Embeddings. No es más que un vecor que representa cada palabra, esta es la intención del siguiente modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramasInstalados\\Anaconda3\\envs\\mt\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 81s 736us/step - loss: 2.6462 - accuracy: 0.5238 - val_loss: 2.2522 - val_accuracy: 0.5402\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 80s 729us/step - loss: 2.6966 - accuracy: 0.5083 - val_loss: 2.8112 - val_accuracy: 0.5057\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 81s 734us/step - loss: 2.2703 - accuracy: 0.5613 - val_loss: 2.0705 - val_accuracy: 0.5858\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 82s 744us/step - loss: 2.1995 - accuracy: 0.5601 - val_loss: 2.0750 - val_accuracy: 0.5654\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 83s 754us/step - loss: 2.1111 - accuracy: 0.5657 - val_loss: 1.9600 - val_accuracy: 0.5846A: 0s - loss: 2.1112 - accuracy: 0.56\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 83s 756us/step - loss: 1.9642 - accuracy: 0.5893 - val_loss: 1.8962 - val_accuracy: 0.5961\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 85s 767us/step - loss: 2.8479 - accuracy: 0.4800 - val_loss: 2.2559 - val_accuracy: 0.5890\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 85s 768us/step - loss: 1.8894 - accuracy: 0.6153 - val_loss: 1.7120 - val_accuracy: 0.6248\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 85s 768us/step - loss: 1.7719 - accuracy: 0.6206 - val_loss: 1.6987 - val_accuracy: 0.6236\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 85s 775us/step - loss: 1.7228 - accuracy: 0.6222 - val_loss: 1.7093 - val_accuracy: 0.61762s \n",
      "nous nous est parfois calme calme l' l' novembre novembre novembre novembre est en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    number_units = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size, output_dim=20))\n",
    "    model.add(GRU(number_units, return_sequences=True, input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0014, decay=0.00004),\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "    return model\n",
    "#tests.test_embed_model(embed_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, 21)\n",
    "\n",
    "\n",
    "#simple_rnn_model = simple_model(\n",
    "#    tmp_x.shape,\n",
    "#    preproc_french_sentences.shape[1],\n",
    "#    len(english_tokenizer.word_index)+1,\n",
    "#    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "#model = embed_model(tmp_x.shape,\n",
    "#                    max_french_sequence_length,\n",
    "#                    english_vocab_size,\n",
    "#                    french_vocab_size)\n",
    "\n",
    "model = embed_model(tmp_x.shape,\n",
    "                    preproc_french_sentences.shape[1],\n",
    "                    len(english_tokenizer.word_index)+1,\n",
    "                    len(french_tokenizer.word_index)+1)\n",
    "# TODO: Train the neural network\n",
    "model.fit(tmp_x, \n",
    "          preproc_french_sentences, \n",
    "          batch_size=256, \n",
    "          epochs=10, \n",
    "          validation_split=0.2)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis\n",
    "En nuestro primer Embeddin vemos un resultado menor al anterior, con un accuracy del 60% aproximadamente. Como se analizó en el primero modelo se especificó un batchsize de 1024, ahora se redujo a 254, pero parce que esta medida menora el desempeño del resultado final.\n",
    "\n",
    "Aumentamos además el lerning rate a 0,005. Además, se utilizará la función de activación `relu` en primera instancia y luego `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 21, 256)           51200     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 21, 256)           393984    \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 1,063,002\n",
      "Trainable params: 1,063,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramasInstalados\\Anaconda3\\envs\\mt\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 380s 3ms/step - loss: 0.9108 - accuracy: 0.7648 - val_loss: 0.3378 - val_accuracy: 0.8882\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 375s 3ms/step - loss: 0.2992 - accuracy: 0.9007 - val_loss: 0.2427 - val_accuracy: 0.9172\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.2403 - accuracy: 0.9189 - val_loss: 0.2161 - val_accuracy: 0.9257\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.2189 - accuracy: 0.9253 - val_loss: 0.2041 - val_accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.2052 - accuracy: 0.9292 - val_loss: 0.1958 - val_accuracy: 0.9322\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.2009 - accuracy: 0.9305 - val_loss: 0.1938 - val_accuracy: 0.9333\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.1924 - accuracy: 0.9327 - val_loss: 0.1880 - val_accuracy: 0.9346\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.1900 - accuracy: 0.9335 - val_loss: 0.1890 - val_accuracy: 0.9353\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 388s 4ms/step - loss: 0.1950 - accuracy: 0.9321 - val_loss: 0.1934 - val_accuracy: 0.9347\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 385s 3ms/step - loss: 0.1904 - accuracy: 0.9336 - val_loss: 0.1942 - val_accuracy: 0.9333\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(GRU(256, return_sequences=True))    \n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_embed_model(embed_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# TODO: Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=512, epochs=10, validation_split=0.2)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis\n",
    "Con los cambios mencianos se ha conseguido una mejora notable, esto a llevado a obtener un accuracy del 93%. Si observamos la predicción, es mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3: RNN Bidereccional (IMPLEMENTACION)\n",
    "![RNN](images/bidirectional.png)\n",
    "Una restricción de una red neuronal es, que no puede ver las entradas futuras, por ello, sus resultados se basan en entradas pasadas. Aquí es donde entra una red neuronal bidirección. Esta arquitectura permite ver los datos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 116s 1ms/step - loss: 5.5674 - accuracy: 0.4202 - val_loss: 5.7768 - val_accuracy: 0.3944\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 108s 979us/step - loss: 5.4857 - accuracy: 0.4095 - val_loss: 5.3599 - val_accuracy: 0.4125\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 109s 992us/step - loss: 5.3804 - accuracy: 0.4013 - val_loss: 5.3357 - val_accuracy: 0.4116\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 107s 974us/step - loss: 5.3308 - accuracy: 0.3905 - val_loss: 5.4855 - val_accuracy: 0.3995\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 108s 978us/step - loss: 5.3588 - accuracy: 0.3986 - val_loss: 5.2302 - val_accuracy: 0.4021\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 109s 984us/step - loss: 5.1726 - accuracy: 0.4089 - val_loss: 5.0072 - val_accuracy: 0.4223\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 107s 972us/step - loss: 4.9141 - accuracy: 0.4425 - val_loss: 4.4003 - val_accuracy: 0.4655\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 108s 980us/step - loss: 4.9075 - accuracy: 0.4462 - val_loss: 4.9937 - val_accuracy: 0.4089\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 110s 997us/step - loss: 4.5169 - accuracy: 0.4177 - val_loss: 4.3639 - val_accuracy: 0.4184\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 112s 1ms/step - loss: 4.3687 - accuracy: 0.4152 - val_loss: 4.3235 - val_accuracy: 0.4142\n",
      "le le et et le le le et et et le et le <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    number_units = 100\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(number_units, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0014, decay=0.00003),\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "    return model \n",
    "    \n",
    "tmp_x = pad(preproc_english_sentences, 21)\n",
    "tmp_x = tmp_x.reshape((-1, 21, 1))\n",
    "#model = bd_model(tmp_x.shape,\n",
    "#                 max_french_sequence_length,\n",
    "#                 english_vocab_size,\n",
    "#                 french_vocab_size)\n",
    "\n",
    "model = bd_model(tmp_x.shape,\n",
    "                 preproc_french_sentences.shape[1],\n",
    "                 len(english_tokenizer.word_index)+1,\n",
    "                 len(french_tokenizer.word_index)+1)\n",
    "\n",
    "#preproc_french_sentences.shape[1],\n",
    "#                    len(english_tokenizer.word_index)+1,\n",
    "#                    len(french_tokenizer.word_index)+1\n",
    "\n",
    "\n",
    "\n",
    "model.fit(tmp_x, \n",
    "          preproc_french_sentences, \n",
    "          batch_size=500, \n",
    "          epochs=10, \n",
    "          validation_split=0.2)\n",
    "\n",
    "\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "# TODO: Train and Print prediction(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 21, 256)           51200     \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 21, 256)           393984    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 1,063,002\n",
      "Trainable params: 1,063,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramasInstalados\\Anaconda3\\envs\\mt\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 392s 4ms/step - loss: 0.8995 - accuracy: 0.7686 - val_loss: 0.3298 - val_accuracy: 0.8903\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 388s 4ms/step - loss: 0.2997 - accuracy: 0.9003 - val_loss: 0.2396 - val_accuracy: 0.9181\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 389s 4ms/step - loss: 0.2389 - accuracy: 0.9193 - val_loss: 0.2131 - val_accuracy: 0.9273\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 386s 3ms/step - loss: 0.2162 - accuracy: 0.9261 - val_loss: 0.1993 - val_accuracy: 0.9310\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 387s 4ms/step - loss: 0.2037 - accuracy: 0.9296 - val_loss: 0.1923 - val_accuracy: 0.9328\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 387s 4ms/step - loss: 0.1974 - accuracy: 0.9311 - val_loss: 0.1950 - val_accuracy: 0.9328\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 388s 4ms/step - loss: 0.1951 - accuracy: 0.9320 - val_loss: 0.1921 - val_accuracy: 0.9341\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 389s 4ms/step - loss: 0.1899 - accuracy: 0.9333 - val_loss: 0.1839 - val_accuracy: 0.9357\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 387s 4ms/step - loss: 0.1841 - accuracy: 0.9350 - val_loss: 0.1931 - val_accuracy: 0.9346\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 388s 4ms/step - loss: 0.1881 - accuracy: 0.9340 - val_loss: 0.1869 - val_accuracy: 0.9360\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_bd_model(bd_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=512, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis.\n",
    "Nuevamente, en nuestro primer intento obtuvimos un resultado muy precario. Por ello se hizo unas modificaciones del modelo manteniendo un batchsize de 512. De igual manera que en la red neuronal anterior, se añadío una capa con la función de activación `relu` mejorando notablemente el accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder (IMPLEMENTATION)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    learning_rate = 100\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(128, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0014, decay=0.00003),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
    "\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_11 (GRU)                 (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 21, 256)           393984    \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 1,209,946\n",
      "Trainable params: 1,209,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 633s 6ms/step - loss: 2.0703 - accuracy: 0.5238 - val_loss: 1.4995 - val_accuracy: 0.6057\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 623s 6ms/step - loss: 1.4048 - accuracy: 0.6147 - val_loss: 1.2848 - val_accuracy: 0.6462\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 620s 6ms/step - loss: 1.2777 - accuracy: 0.6433 - val_loss: 1.2023 - val_accuracy: 0.6628\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 625s 6ms/step - loss: 1.1961 - accuracy: 0.6594 - val_loss: 1.0989 - val_accuracy: 0.6814\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 627s 6ms/step - loss: 1.1312 - accuracy: 0.6717 - val_loss: 1.0639 - val_accuracy: 0.6874\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 624s 6ms/step - loss: 1.0732 - accuracy: 0.6858 - val_loss: 1.0216 - val_accuracy: 0.6974\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 634s 6ms/step - loss: 1.0426 - accuracy: 0.6951 - val_loss: 0.9766 - val_accuracy: 0.7136\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 638s 6ms/step - loss: 0.9808 - accuracy: 0.7108 - val_loss: 0.9176 - val_accuracy: 0.7246\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 643s 6ms/step - loss: 0.9382 - accuracy: 0.7185 - val_loss: 0.9137 - val_accuracy: 0.7217\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 650s 6ms/step - loss: 0.9183 - accuracy: 0.7207 - val_loss: 0.8514 - val_accuracy: 0.7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16a4c55f388>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], go_backwards=True))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#tests.test_encdec_model(encdec_model)\n",
    "\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train and Print prediction(s)\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=512, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois chaud en mois de il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')\n",
    "# TODO: Train the final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    tmp_X = pad(preproc_english_sentences)\n",
    "    \n",
    "    model = model_final(tmp_X.shape, \n",
    "                        preproc_french_sentences.shape[1],\n",
    "                        len(english_tokenizer.word_index)+1,\n",
    "                        len(french_tokenizer.word_index)+1)\n",
    "    \n",
    "    model.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 15, 128)           25600     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 21, 346)           177498    \n",
      "=================================================================\n",
      "Total params: 827,738\n",
      "Trainable params: 827,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramasInstalados\\Anaconda3\\envs\\mt\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 368s 3ms/step - loss: 1.9041 - accuracy: 0.5559 - val_loss: 1.1766 - val_accuracy: 0.6718\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 349s 3ms/step - loss: 1.0875 - accuracy: 0.6894 - val_loss: 0.8644 - val_accuracy: 0.7419\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 353s 3ms/step - loss: 0.8228 - accuracy: 0.7474 - val_loss: 0.7340 - val_accuracy: 0.7696\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 362s 3ms/step - loss: 0.6768 - accuracy: 0.7868 - val_loss: 0.5226 - val_accuracy: 0.8339\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 376s 3ms/step - loss: 0.5005 - accuracy: 0.8414 - val_loss: 0.3645 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 383s 3ms/step - loss: 0.3998 - accuracy: 0.8752 - val_loss: 0.3539 - val_accuracy: 0.8881\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 400s 4ms/step - loss: 0.2914 - accuracy: 0.9114 - val_loss: 0.2042 - val_accuracy: 0.9397\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 415s 4ms/step - loss: 0.2688 - accuracy: 0.9181 - val_loss: 0.1607 - val_accuracy: 0.9531\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 433s 4ms/step - loss: 0.1953 - accuracy: 0.9411 - val_loss: 0.1376 - val_accuracy: 0.9592\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 450s 4ms/step - loss: 0.1689 - accuracy: 0.9489 - val_loss: 0.1314 - val_accuracy: 0.9601\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    model.fit(x, y, batch_size=512, epochs=10, validation_split=0.2)\n",
    "\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois parfois en l' et il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the html\n",
    "\n",
    "**Save your notebook before running the next cell to generate the HTML output.** Then submit your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook machine_translationv1.ipynb to html',\n",
       " '[NbConvertApp] Writing 357192 bytes to machine_translationv1.html',\n",
       " '[NbConvertApp] Converting notebook machine_translationv2 - copia.ipynb to html',\n",
       " '[NbConvertApp] Writing 535003 bytes to machine_translationv2 - copia.html',\n",
       " '[NbConvertApp] Converting notebook machine_translationv2.ipynb to html',\n",
       " '[NbConvertApp] Writing 545081 bytes to machine_translationv2.html']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save before you run this cell!\n",
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "[1]Pipeline (Segmentación), http://ciecfie.epn.edu.ec/wss/VirtualDirectories/80/pag_personales/PChico/Materiales_Micros/pipeline-intro.pdf \n",
    "[2]Getting started with the Keras Sequential model https://keras.io/getting-started/sequential-model-guide/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
